#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\begin_preamble
\usepackage{indentfirst}
\usepackage[useregional]{datetime2}
\end_preamble
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family rmdefault
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures true
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\float_placement !tph
\paperfontsize 11
\spacing single
\use_hyperref false
\papersize default
\use_geometry true
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\use_minted 0
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 1in
\topmargin 1in
\rightmargin 1in
\bottommargin 1in
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Using transfer learning to develop a deep neural network based on PhaseNet
 
\end_layout

\begin_layout Section
Why do we use transfer learning?
\begin_inset CommandInset label
LatexCommand label
name "sec:transfer_learning"

\end_inset


\end_layout

\begin_layout Standard
Nowadays, many deep learning algorithms work well only under a common assumption
 even if their models achieved a significant precision with their data set.
 If we want to apply these methods for a newly collected training data that
 is drawn from another feature space and another distribution, then the
 models need to be rebuilt from scratch to learn about all features of the
 new data.
 For instance, PhaseNet used the data based on 
\shape italic
Northern California Earthquake Data Center Catalog
\shape default
 (NCEDC 2014) that has a lot of different assumptions with our data set
 as: collected locations, collected methods and measurement conditions,
 etc., thus we can not just apply the model trained on the NCEDC data to
 predict for our data.
 Either we retrain the model on our own data, or we have to look for a technique
 which allow us to reuse the features the model has learnt in the past.
 In reality, it is much expensive to recollect the needed training data
 in order to rebuild the model.
 The so-called 
\begin_inset Quotes eld
\end_inset

transfer learning
\begin_inset Quotes erd
\end_inset

 is a method where a model trained on one task is repurposed on a second
 related task in order to improve performance on this second task.
 
\end_layout

\begin_layout Standard
In some cases, the transfer learning help the training to have a higher
 start and so, improve the learning performance.
 On the other hand, if the tasks are too dissimilar, the fact that we use
 transfer learning may hinder performance of the training.
 In the experiments shown in 
\begin_inset CommandInset citation
LatexCommand cite
key "Rosenstein05totransfer"
literal "false"

\end_inset

, they compared performances of the training in three cases (Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "transfer_or_not"
plural "false"
caps "false"
noprefix "false"

\end_inset

):
\end_layout

\begin_layout Enumerate
No transfer training (B-only): they train a prediction model only with the
 data set B;
\end_layout

\begin_layout Enumerate
Similar transfer training: they train the model on data B using the transfer
 learning with the pre-trained model on data A (similar with B).
\end_layout

\begin_layout Enumerate
Dissimilar transfer training: the pre-trained model on the dissimilar data
 A is used to transfer the prediction model on data B.
\end_layout

\begin_layout Standard
\noindent
\align center
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\noindent
\align center
\begin_inset Graphics
	filename figs/transfer_or_not.png
	scale 50

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Effects of transfer learning in three cases
\end_layout

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "transfer_or_not"

\end_inset


\end_layout

\begin_layout Plain Layout
\align center

\shape italic
Taken from 
\shape default

\begin_inset CommandInset citation
LatexCommand cite
key "Rosenstein05totransfer"
literal "false"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Section
How does it work?
\end_layout

\begin_layout Subsection
Notations and definitions
\end_layout

\begin_layout Standard
Before surveying mathematical definition of transfer learning, we first
 define the following notations:
\end_layout

\begin_layout Itemize
A 
\shape italic
domain 
\begin_inset Formula $\mathcal{D=}\{\mathcal{X},P(X)\}$
\end_inset

 
\shape default
consists of 2 components: a feature space 
\shape italic

\begin_inset Formula $\mathcal{X}$
\end_inset

 
\shape default
and a marginal probability distribution 
\begin_inset Formula $P(X)$
\end_inset

 where 
\begin_inset Formula $X=\{x_{1},...,x_{n}\}\in\mathcal{X}$
\end_inset

.
 
\end_layout

\begin_layout Itemize
A 
\shape italic
task 
\begin_inset Formula $\mathcal{T=}\{\mathcal{Y},f(.)\}$
\end_inset


\shape default
 linked to domain 
\shape italic

\begin_inset Formula $\mathcal{D}$
\end_inset


\shape default
 consists of 2 components: a label space 
\begin_inset Formula $\mathcal{Y}$
\end_inset

 and an objective predictive function 
\begin_inset Formula $f(.)$
\end_inset

, which can be learnt from the training data and consists of the data instance
 
\begin_inset Formula $x_{i}\in\mathcal{X}$
\end_inset

 and the corresponding output 
\begin_inset Formula $y_{i}\in\mathcal{Y}$
\end_inset

.
 Probabilistically, we can write 
\begin_inset Formula $f(x)$
\end_inset

 as 
\begin_inset Formula $P(y|x)$
\end_inset

 where 
\begin_inset Formula $x$
\end_inset

 is a new instance we want to predict and 
\begin_inset Formula $y$
\end_inset

 is the corresponding prediction.
 
\end_layout

\begin_layout Standard
Transfer learning in 
\begin_inset CommandInset citation
LatexCommand cite
key "pan_yang_transfer_learning"
literal "false"

\end_inset

 is defined as below:
\end_layout

\begin_layout Quote
\begin_inset Quotes eld
\end_inset


\shape italic
Given a source domain 
\begin_inset Formula $\mathcal{D}_{S}$
\end_inset

 and learning task 
\begin_inset Formula $T_{S}$
\end_inset

, a target domain 
\begin_inset Formula $\mathcal{D}_{T}$
\end_inset

 and learning task 
\begin_inset Formula $\mathcal{T}_{T}$
\end_inset

, transfer learning aims to help improve the learning of the target predictive
 function 
\begin_inset Formula $f_{T}(·)$
\end_inset

 in 
\begin_inset Formula $\mathcal{D}_{T}$
\end_inset

 using the knowledge in 
\begin_inset Formula $\mathcal{D}_{S}$
\end_inset

 and 
\begin_inset Formula $\mathcal{T}_{S}$
\end_inset

, where 
\begin_inset Formula $\mathcal{D}_{S}\neq\mathcal{D}_{T}$
\end_inset

, or 
\begin_inset Formula $\mathcal{T}_{S}\neq\mathcal{T}_{T}$
\end_inset

.
\shape default

\begin_inset Quotes erd
\end_inset


\end_layout

\begin_layout Standard
Based on this definition, we can categorize transfer learning under three
 settings (see in 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:Categories"
plural "false"
caps "false"
noprefix "false"

\end_inset

) depending on different situations between 
\shape italic

\begin_inset Formula $\mathcal{D}_{S}$
\end_inset


\shape default
 and
\shape italic
 
\begin_inset Formula $\mathcal{D}_{T}$
\end_inset

, 
\shape default
and between
\shape italic
 
\begin_inset Formula $\mathcal{T}_{S}$
\end_inset


\shape default
 and
\shape italic
 
\begin_inset Formula $\mathcal{T}_{T}$
\end_inset


\shape default
.
 
\end_layout

\begin_layout Subsection
Categorization
\begin_inset CommandInset label
LatexCommand label
name "subsec:Categories"

\end_inset


\end_layout

\begin_layout Standard
We categorize transfer learning based on different situations between the
 source and target domains and tasks as:
\end_layout

\begin_layout Itemize

\shape italic
Inductive transfer learning
\shape default
: the target and source tasks are different from each other 
\shape italic

\begin_inset Formula $\mathcal{T}_{S}\neq\mathcal{T}_{T}$
\end_inset


\shape default
.
 For example, in the case that we hope to predict for another output parameter
 instead of three parameters (S-waves, P-waves and noises) in PhaseNet.
 In this case, the algorithms try to utilize the inductive biases of the
 source domain 
\shape italic

\begin_inset Formula $\mathcal{D}_{S}$
\end_inset

 
\shape default
in order to help to induce an objective predictive model 
\shape italic

\begin_inset Formula $f_{T}(·)$
\end_inset


\shape default
.
 Depending upon different situations of labeled and unlabeled data in the
 source domain, this can be further divided into two subcategories.
 Either a lot of labeled data in the source domain are available (similar
 to multitask learning first proposed in 
\begin_inset CommandInset citation
LatexCommand cite
key "MTL_Caruana"
literal "false"

\end_inset

, yet the network in multitask learning tries to learn the target and source
 task simultaneously instead of learning the target task by transferring
 knowledge from the source task), or no labeled data in the source domain
 are available (similar to self-taught learning first proposed in 
\begin_inset CommandInset citation
LatexCommand cite
key "STL_raina"
literal "false"

\end_inset

, where the label spaces between the source and target domains may be different).
\end_layout

\begin_layout Itemize

\shape italic
Transductive transfer learning
\shape default
: the source and target domains are different 
\shape italic

\begin_inset Formula $\mathcal{D}_{S}\neq\mathcal{D}_{T}$
\end_inset

,
\shape default
 but the source and target tasks are the same 
\shape italic

\begin_inset Formula $\mathcal{T}_{S}\approx\mathcal{T}_{T}$
\end_inset

.
 
\shape default
The condition 
\shape italic

\begin_inset Formula $\mathcal{D}_{S}\neq\mathcal{D}_{T}$
\end_inset


\shape default
 implies that either 
\begin_inset Formula $\mathcal{X}_{S}\neq\mathcal{X}_{T}$
\end_inset

, or 
\begin_inset Formula $P(X_{S})\neq P(X_{T})$
\end_inset

.
 In the first case, the feature spaces between the source and target domains
 are different (
\begin_inset Formula $\mathcal{X}_{S}\neq\mathcal{X}_{T}$
\end_inset

).
 For example, if the seismograms in our new data are based on components
 which are totally different from those in PhaseNet.
 The later case assumes that the feature spaces between domains are the
 same but the marginal probability distributions of the input data are different
 
\begin_inset Formula $P(X_{S})\neq P(X_{T})$
\end_inset

.
 For example, if out data is collected with the same components but from
 different criteria and conditions.
 
\end_layout

\begin_layout Itemize

\shape italic
Unsupervised transfer learning
\shape default
: similar to inductive transfer learning, the target and source tasks are
 different but related to each other.
 Nevertheless, unsupervised transfer learning focus on solving unsupervised
 learning tasks in the target domain.
\end_layout

\begin_layout Subsection
Training on new data set from pre-trained model in PhaseNet
\begin_inset CommandInset label
LatexCommand label
name "subsec:Training-on-new-phasenet"

\end_inset


\end_layout

\begin_layout Standard
Actually, PhaseNet allows to learn a new model for a new data set without
 training from scratch by using the command: 
\end_layout

\begin_layout Verbatim
$ python run.py --mode=train --model_dir=pre_trained --train_dir=waves_train
\end_layout

\begin_layout Standard
\noindent
where 
\family sans
pre_trained
\family default
 is the model which is trained with the NCEDC data and 
\family sans
waves_train
\family default
 is our new data.
 Briefly, the process is that, in the first time, Keras will load the pre-traine
d model stored in 
\family sans
pre_trained
\family default
, that means all required model weights are loaded from this.
 Subsequently, the network tries to learn new features of the new data using
 the knowledge of pre-trained model instead of initializing the model with
 random weights.
 Finally, the new model can be improved performance and adapt to the new
 data set using the knowledge in the past.
\end_layout

\begin_layout Standard
However, the training on new data set of PhaseNet requires the same data
 and target task, that means we must have 
\shape italic

\begin_inset Formula $\mathcal{D}_{S}=\mathcal{D}_{T}$
\end_inset

 
\shape default
and
\shape italic
 
\begin_inset Formula $\mathcal{T}_{S}=\mathcal{T}_{T}$
\end_inset

.
 
\shape default
These conditions are obviously not met in our case, so we need to add a
 real transfer learning in the code in order to adapt to our own data set
 and our issues.
 
\end_layout

\begin_layout Section
Transfer learning for PhaseNet
\end_layout

\begin_layout Standard
(This section will be presented when I start coding)
\end_layout

\begin_layout Standard
Explain that we do not use transfer because the data diff..
 loss does not decrease while using transfer (commence a un loss tres bas
 mais il ne decroit pas donc il tombe dans un min local ...).
 
\end_layout

\begin_layout Standard
\begin_inset CommandInset bibtex
LatexCommand bibtex
btprint "btPrintCited"
bibfiles "references"
options "siam"

\end_inset


\end_layout

\end_body
\end_document
