@InProceedings{SelimSaleh2011,
  author="Selim Saleh, Mahmoud Mohamed",
  editor="Gupta, Harsh K.",
  title="Body Waves",
  booktitle="Encyclopedia of Solid Earth Geophysics",
  year="2011",
  publisher="Springer Netherlands",
  address="Dordrecht",
  pages="29--35",
}

@article{berozaPhaseNet,
  author = {Zhu, Weiqiang and Beroza, Gregory C},
  title = "{PhaseNet: a deep-neural-network-based seismic arrival-time picking method}",
  journal = {Geophysical Journal International},
  volume = {216},
  number = {1},
  pages = {261-273},
  year = {2018},
  month = {10},
  abstract = "{As the number of seismic sensors grows, it is becoming increasingly difficult for analysts to pick seismic phases manually and comprehensively, yet such efforts are fundamental to earthquake monitoring. Despite years of improvements in automatic phase picking, it is difficult to match the performance of experienced analysts. A more subtle issue is that different seismic analysts may pick phases differently, which can introduce bias into earthquake locations. We present a deep-neural-network-based arrival-time picking method called “PhaseNet” that picks the arrival times of both P and S waves. Deep neural networks have recently made rapid progress in feature learning, and with sufficient training, have achieved super-human performance in many applications. PhaseNet uses three-component seismic waveforms as input and generates probability distributions of P arrivals, S arrivals and noise as output. We engineer PhaseNet such that peaks in the probability distributions provide accurate arrival times for both P and S waves. PhaseNet is trained on the prodigious available data set provided by analyst-labelled P and S arrival times from the Northern California Earthquake Data Center. The data set we use contains more than 700 000 waveform samples extracted from over 30 yr of earthquake recordings. We demonstrate that PhaseNet achieves much higher picking accuracy and recall rate than existing methods when applied to the waveforms of known earthquakes, which has the potential to increase the number of S-wave observations dramatically over what is currently available. This will enable both improved locations and improved shear wave velocity models.}",
  issn = {0956-540X},
  doi = {10.1093/gji/ggy423},
  url = {https://doi.org/10.1093/gji/ggy423},
  eprint = {https://academic.oup.com/gji/article-pdf/216/1/261/26329430/ggy423.pdf},
}

@article{fangshu2019,
  author = {Fangshu Yang and Jianwei Ma},
  title = {Deep-learning inversion: A next-generation seismic velocity model building method},
  journal = {GEOPHYSICS},
  volume = {84},
  number = {4},
  pages = {R583-R599},
  year = {2019},
  doi = {10.1190/geo2018-0249.1},
  URL = { https://doi.org/10.1190/geo2018-0249.1},
  eprint = {https://doi.org/10.1190/geo2018-0249.1},
}

@article{UnetDL,
  author    = {Olaf Ronneberger and
               Philipp Fischer and
               Thomas Brox},
  title     = {U-Net: Convolutional Networks for Biomedical Image Segmentation},
  journal   = {CoRR},
  volume    = {abs/1505.04597},
  year      = {2015},
  url       = {http://arxiv.org/abs/1505.04597},
  archivePrefix = {arXiv},
  eprint    = {1505.04597},
  timestamp = {Mon, 13 Aug 2018 16:46:52 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/RonnebergerFB15.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

 @MISC{skipconnection,
  author =     "Adaloglou, Nikolas",
  title =        "Intuitive Explanation of Skip Connections in Deep Learning",
  editor =       "theaisummer.com",
  month =        "3",
  year =         "2020",
  url =          "\url{https://theaisummer.com/skip-connections/}",
  note =         "[Online; posted 23-Mar-2020]",
 }

@InProceedings{skipconnectImportance,
  author="Drozdzal, Michal and Vorontsov, Eugene and Chartrand, Gabriel and Kadoury, Samuel and Pal, Chris",
  editor="Carneiro, Gustavo and Mateus, Diana and Peter, Lo{\"i}c and Bradley, Andrew and Tavares, Jo{\~a}o Manuel R. S. and Belagiannis, Vasileios and Papa, Jo{\~a}o Paulo and Nascimento, Jacinto C. and Loog, Marco and Lu, Zhi and Cardoso, Jaime S. and Cornebise, Julien",
  title="The Importance of Skip Connections in Biomedical Image Segmentation",
  booktitle="Deep Learning and Data Labeling for Medical Applications",
  year="2016",
  publisher="Springer International Publishing",
  address="Cham",
  pages="179--187",
  isbn="978-3-319-46976-8"
}

@InProceedings{Long_2015_CVPR,
  author = {Long, Jonathan and Shelhamer, Evan and Darrell, Trevor},
  title = {Fully Convolutional Networks for Semantic Segmentation},
  booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  month = {June},
  year = {2015}
}

@InProceedings{ResNet_He_2016_CVPR,
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  title = {Deep Residual Learning for Image Recognition},
  booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  month = {June},
  year = {2016}
}

@InProceedings{DenseNet_Huang_2017_CVPR,
  author = {Huang, Gao and Liu, Zhuang and van der Maaten, Laurens and Weinberger, Kilian Q.},
  title = {Densely Connected Convolutional Networks},
  booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  month = {July},
  year = {2017}
}

 @MISC{nikolas_DLMIS,
  author =       "Adaloglou, Nikolas",
  title =        "Deep learning in medical imaging: 3D medical image segmentation with PyTorch",
  editor =       "theaisummer.com",
  month =        "4",
  year =         "2020",
  url =          "\url{https://theaisummer.com/medical-image-deep-learning/}",
  note =         "[Online; posted 02-Apr-2020]",
 }

 @MISC{deconvolution_Chris,
  author =       "Christian Versloot",
  title =        "Understanding transposed convolutions",
  editor =       "machinecurve.com",
  month =        "9",
  year =         "2019",
  url =          "\url{https://www.machinecurve.com/index.php/2019/09/29/understanding-transposed-convolutions/}",
  note =         "[Online; posted 29-Sep-2019]",
 }
 
 @INPROCEEDINGS{Rosenstein05totransfer,
  author = "Michael T. Rosenstein and Zvika Marx and Leslie Pack Kaelbling and Thomas G. Dietterich",
  title = "To transfer or not to transfer",
  booktitle = "In NIPS'05 Workshop, Inductive Transfer: 10 Years Later",
  year = "2005"
}

@ARTICLE{pan_yang_transfer_learning,
  author={S. J. {Pan} and Q. {Yang}},
  journal={IEEE Transactions on Knowledge and Data Engineering}, 
  title={A Survey on Transfer Learning}, 
  year={2010},
  volume={22},
  number={10},
  pages={1345-1359},
  abstract={A major assumption in many machine learning and data mining algorithms is that the training and future data must be in the same feature space and have the same distribution. However, in many real-world applications, this assumption may not hold. For example, we sometimes have a classification task in one domain of interest, but we only have sufficient training data in another domain of interest, where the latter data may be in a different feature space or follow a different data distribution. In such cases, knowledge transfer, if done successfully, would greatly improve the performance of learning by avoiding much expensive data-labeling efforts. In recent years, transfer learning has emerged as a new learning framework to address this problem. This survey focuses on categorizing and reviewing the current progress on transfer learning for classification, regression, and clustering problems. In this survey, we discuss the relationship between transfer learning and other related machine learning techniques such as domain adaptation, multitask learning and sample selection bias, as well as covariate shift. We also explore some potential future issues in transfer learning research.},
  keywords={knowledge engineering;learning by example;optimisation;unsupervised learning;machine learning;data mining;knowledge transfer;inductive transfer learning;transductive transfer learning;unsupervised transfer learning;Machine learning;Training data;Data mining;Knowledge transfer;Space technology;Knowledge engineering;Machine learning algorithms;Labeling;Learning systems;Testing;Transfer learning;survey;machine learning;data mining.},
  doi={10.1109/TKDE.2009.191},
  ISSN={1558-2191},
  month={Oct},
}

@article{MTL_Caruana,
  author = {Caruana, Rich},
  title = {Multitask Learning},
  year = {1997},
  issue_date = {July 1997},
  publisher = {Kluwer Academic Publishers},
  address = {USA},
  volume = {28},
  number = {1},
  issn = {0885-6125},
  url = {https://doi.org/10.1023/A:1007379606734},
  doi = {10.1023/A:1007379606734},
  abstract = {Multitask Learning is an approach to inductive transfer that improves
generalization by using the domain information contained in the
training signals of related tasks as an inductive bias. It
does this by learning tasks in parallel while using a shared
representation; what is learned for each task can help other tasks be
learned better. This paper reviews prior work on MTL, presents new
evidence that MTL in backprop nets discovers task relatedness without
the need of supervisory signals, and presents new results for MTL
with k-nearest neighbor and kernel regression. In this paper we
demonstrate multitask learning in three domains. We explain how
multitask learning works, and show that there are many opportunities
for multitask learning in real domains. We present an algorithm and
results for multitask learning with case-based methods like k-nearest
neighbor and kernel regression, and sketch an algorithm for multitask
learning in decision trees. Because multitask learning works, can be
applied to many different kinds of domains, and can be used with
different learning algorithms, we conjecture there will be many
opportunities for its use on real-world problems.},
  journal = {Mach. Learn.},
  month = jul,
  pages = {41--75},
  numpages = {35},
  keywords = {kernel regression, generalization, parallel transfer, k-nearest neighbor, inductive transfer, multitask learning, backpropagation, supervised learning},
}

@inproceedings{STL_raina,
  author = {Raina, Rajat and Battle, Alexis and Lee, Honglak and Packer, Benjamin and Ng, Andrew Y.},
  title = {Self-Taught Learning: Transfer Learning from Unlabeled Data},
  year = {2007},
  isbn = {9781595937933},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/1273496.1273592},
  doi = {10.1145/1273496.1273592},
  abstract = {We present a new machine learning framework called "self-taught learning" for using unlabeled data in supervised classification tasks. We do not assume that the unlabeled data follows the same class labels or generative distribution as the labeled data. Thus, we would like to use a large number of unlabeled images (or audio samples, or text documents) randomly downloaded from the Internet to improve performance on a given image (or audio, or text) classification task. Such unlabeled data is significantly easier to obtain than in typical semi-supervised or transfer learning settings, making self-taught learning widely applicable to many practical learning problems. We describe an approach to self-taught learning that uses sparse coding to construct higher-level features using the unlabeled data. These features form a succinct input representation and significantly improve classification performance. When using an SVM for classification, we further show how a Fisher kernel can be learned for this representation.},
  booktitle = {Proceedings of the 24th International Conference on Machine Learning},
  pages = {759--766},
  numpages = {8},
  location = {Corvalis, Oregon, USA},
  series = {ICML '07},
}

@article{phasenet_and_transfer,
  author = {Chai, Chengping and Maceira, Monica and Santos-Villalobos, Hector J. and Venkatakrishnan, Singanallur V. and Schoenball, Martin and Zhu, Weiqiang and Beroza, Gregory C. and Thurber, Clifford and EGS Collab Team},
  title = {Using a Deep Neural Network and Transfer Learning to Bridge Scales for Seismic Phase Picking},
  journal = {Geophysical Research Letters},
  volume = {47},
  number = {16},
  pages = {e2020GL088651},
  keywords = {machine learning, transfer learning, neural network, seismic tomography, geothermal, seismic phase picking},
  doi = {https://doi.org/10.1029/2020GL088651},
  url = {https://agupubs.onlinelibrary.wiley.com/doi/abs/10.1029/2020GL088651},
  eprint = {https://agupubs.onlinelibrary.wiley.com/doi/pdf/10.1029/2020GL088651},
  note = {e2020GL088651 2020GL088651},
abstract = {Abstract The important task of tracking seismic activity requires both sensitive detection and accurate earthquake location. Approximate earthquake locations can be estimated promptly and automatically; however, accurate locations depend on precise seismic phase picking, which is a laborious and time-consuming task. We adapted a deep neural network (DNN) phase picker trained on local seismic data to mesoscale hydraulic fracturing experiments. We designed a novel workflow, transfer learning-aided double-difference tomography, to overcome the 3 orders of magnitude difference in both spatial and temporal scales between our data and data used to train the original DNN. Only 3,500 seismograms (0.45\% of the original DNN data) were needed to retrain the original DNN model successfully. The phase picks obtained with transfer-learned model are at least as accurate as the analyst's and lead to improved event locations. Moreover, the effort required for picking once the DNN is trained is a small fraction of the analyst's.},
  year = {2020}
}

@Software{specfem2d, 
  author = "Komatitsch, D. and Vilotte, J.-P. and Cristini, P. and Labarta, J. and Le Goff, N. and Le Loher, P. and Liu, Q. and Martin, R. and Matzen, R. and Morency, C. and Peter, D. and  Tape, C. and Tromp, J. and Xie, Z.", 
  title="SPECFEM2D v7.0.0 [software]", 
  year="2012", 
  organization="Computational Infrastructure for Geodynamics", 
  optkeywords="SPECFEM2D", 
  doi="http://doi.org/NoDOI", 
  opturl="https://geodynamics.org/cig/software/specfem2d/"
 }
